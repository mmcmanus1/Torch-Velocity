{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch-Velocity: Adaptive Speculative Decoding\n",
    "\n",
    "**An Inference Optimization Engine for Large Language Models**\n",
    "\n",
    "This notebook implements speculative decoding with adaptive lookahead, achieving 1.5-2.5x inference speedups.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "LLM inference is **memory-bandwidth bound**, not compute-bound. During autoregressive generation, we move gigabytes of weights from VRAM to compute units just to predict simple tokens like \"the\" or \"and\".\n",
    "\n",
    "## The Solution\n",
    "\n",
    "**Speculative Decoding** (Leviathan et al., 2023) uses a small draft model to generate K tokens speculatively, then verifies them in a single parallel forward pass through the target model.\n",
    "\n",
    "This implementation adds **adaptive γ** - dynamically adjusting the lookahead length based on acceptance rates.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Colab)\n",
    "# !pip install torch transformers matplotlib tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "# Draft: distilgpt2 (82M params) - fast but less accurate\n",
    "# Target: gpt2-medium (355M params) - slower but higher quality\n",
    "\n",
    "print(\"Loading draft model (distilgpt2)...\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\").to(device).eval()\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "print(\"Loading target model (gpt2-medium)...\")\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\").to(device).eval()\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Use same tokenizer for both (they share vocabulary)\n",
    "tokenizer = target_tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Draft model params: {sum(p.numel() for p in draft_model.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"Target model params: {sum(p.numel() for p in target_model.parameters()) / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: KV Cache Manager\n",
    "\n",
    "The key systems engineering component. Standard implementations recompute attention for the entire sequence on each token. Our `KVCacheManager` maintains pre-allocated tensors with O(1) rollback capability.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "In speculative decoding, if the target model rejects token 3 of 5 drafted tokens, we must \"rollback\" the KV cache to the state after token 2. This requires efficient cache management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KVCache:\n",
    "    \"\"\"Simple KV cache wrapper for a single layer.\"\"\"\n",
    "    key: torch.Tensor\n",
    "    value: torch.Tensor\n",
    "\n",
    "\n",
    "class KVCacheManager:\n",
    "    \"\"\"\n",
    "    Pre-allocated KV cache with O(1) rollback.\n",
    "    \n",
    "    This is the core systems engineering component of speculative decoding.\n",
    "    Instead of dynamically growing/shrinking the cache, we pre-allocate\n",
    "    and track a position pointer that can be rewound on rejection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        head_dim: int,\n",
    "        max_seq_len: int = 2048,\n",
    "        batch_size: int = 1,\n",
    "        dtype: torch.dtype = torch.float16,\n",
    "        device: str = \"cuda\"\n",
    "    ):\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        \n",
    "        # Pre-allocate cache tensors for all layers\n",
    "        # Shape: (batch, n_heads, max_seq_len, head_dim)\n",
    "        self.k_cache = torch.zeros(\n",
    "            (n_layers, batch_size, n_heads, max_seq_len, head_dim),\n",
    "            dtype=dtype, device=device\n",
    "        )\n",
    "        self.v_cache = torch.zeros(\n",
    "            (n_layers, batch_size, n_heads, max_seq_len, head_dim),\n",
    "            dtype=dtype, device=device\n",
    "        )\n",
    "        \n",
    "        # Current sequence length (the \"pointer\")\n",
    "        self.seq_len = 0\n",
    "    \n",
    "    def update(self, layer_idx: int, k_new: torch.Tensor, v_new: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Append new K/V entries and return the full cache for attention.\n",
    "        \n",
    "        Args:\n",
    "            layer_idx: Which transformer layer\n",
    "            k_new: New key tensor, shape (batch, n_heads, new_tokens, head_dim)\n",
    "            v_new: New value tensor, shape (batch, n_heads, new_tokens, head_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (full_k, full_v) for attention computation\n",
    "        \"\"\"\n",
    "        new_tokens = k_new.shape[2]\n",
    "        \n",
    "        # Write new entries at current position\n",
    "        self.k_cache[layer_idx, :, :, self.seq_len:self.seq_len + new_tokens, :] = k_new\n",
    "        self.v_cache[layer_idx, :, :, self.seq_len:self.seq_len + new_tokens, :] = v_new\n",
    "        \n",
    "        # Note: We don't update seq_len here - that's done in commit()\n",
    "        # This allows for speculative updates that can be rolled back\n",
    "        \n",
    "        # Return valid portion\n",
    "        return self.get(layer_idx, include_new=new_tokens)\n",
    "    \n",
    "    def get(self, layer_idx: int, include_new: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Get the valid portion of cache for a layer.\"\"\"\n",
    "        end = self.seq_len + include_new\n",
    "        return (\n",
    "            self.k_cache[layer_idx, :, :, :end, :],\n",
    "            self.v_cache[layer_idx, :, :, :end, :]\n",
    "        )\n",
    "    \n",
    "    def commit(self, n_tokens: int):\n",
    "        \"\"\"Commit n_tokens to the cache (advance the pointer).\"\"\"\n",
    "        self.seq_len += n_tokens\n",
    "    \n",
    "    def rollback(self, n_tokens: int):\n",
    "        \"\"\"\n",
    "        Rollback the cache by n_tokens.\n",
    "        \n",
    "        This is O(1) - we just move the pointer back.\n",
    "        The old data is still there but will be overwritten on next update.\n",
    "        \"\"\"\n",
    "        self.seq_len = max(0, self.seq_len - n_tokens)\n",
    "    \n",
    "    def clone(self) -> 'KVCacheManager':\n",
    "        \"\"\"Create a snapshot of the current cache state.\"\"\"\n",
    "        new_cache = KVCacheManager(\n",
    "            self.n_layers, self.n_heads, self.head_dim,\n",
    "            self.max_seq_len, self.batch_size, self.dtype, self.device\n",
    "        )\n",
    "        new_cache.k_cache = self.k_cache.clone()\n",
    "        new_cache.v_cache = self.v_cache.clone()\n",
    "        new_cache.seq_len = self.seq_len\n",
    "        return new_cache\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the cache to empty.\"\"\"\n",
    "        self.seq_len = 0\n",
    "    \n",
    "    def to_hf_format(self) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:\n",
    "        \"\"\"\n",
    "        Convert to HuggingFace past_key_values format.\n",
    "        Returns tuple of (key, value) for each layer.\n",
    "        \"\"\"\n",
    "        return tuple(\n",
    "            (self.k_cache[i, :, :, :self.seq_len, :],\n",
    "             self.v_cache[i, :, :, :self.seq_len, :])\n",
    "            for i in range(self.n_layers)\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_hf_cache(cls, past_key_values, max_seq_len: int = 2048) -> 'KVCacheManager':\n",
    "        \"\"\"Create a KVCacheManager from HuggingFace past_key_values.\"\"\"\n",
    "        n_layers = len(past_key_values)\n",
    "        batch_size, n_heads, seq_len, head_dim = past_key_values[0][0].shape\n",
    "        device = past_key_values[0][0].device\n",
    "        dtype = past_key_values[0][0].dtype\n",
    "        \n",
    "        cache = cls(n_layers, n_heads, head_dim, max_seq_len, batch_size, dtype, str(device))\n",
    "        \n",
    "        for i, (k, v) in enumerate(past_key_values):\n",
    "            cache.k_cache[i, :, :, :seq_len, :] = k\n",
    "            cache.v_cache[i, :, :, :seq_len, :] = v\n",
    "        \n",
    "        cache.seq_len = seq_len\n",
    "        return cache\n",
    "\n",
    "\n",
    "# Test the KV Cache Manager\n",
    "print(\"Testing KVCacheManager...\")\n",
    "test_cache = KVCacheManager(n_layers=12, n_heads=12, head_dim=64, max_seq_len=512, device=device, dtype=torch.float32)\n",
    "print(f\"Initial seq_len: {test_cache.seq_len}\")\n",
    "\n",
    "# Simulate adding 10 tokens\n",
    "test_cache.commit(10)\n",
    "print(f\"After commit(10): {test_cache.seq_len}\")\n",
    "\n",
    "# Simulate rollback of 3 tokens\n",
    "test_cache.rollback(3)\n",
    "print(f\"After rollback(3): {test_cache.seq_len}\")\n",
    "\n",
    "print(\"KVCacheManager tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 3: Speculative Sampling Algorithm\n\nThe core mathematical insight from Leviathan et al. (2023): we can use **rejection sampling** to accept/reject draft tokens while guaranteeing the output distribution matches the target model exactly.\n\n### The Math\n\nFor each drafted token $x$ with draft probability $p_d(x)$ and target probability $p_t(x)$:\n\n1. Accept with probability $\\min\\left(1, \\frac{p_t(x)}{p_d(x)}\\right)$\n2. If rejected, sample from the \"residual\" distribution: $p_t(x) - p_d(x)$ (normalized)\n\nThis guarantees the final token comes from $p_t$, even though we used $p_d$ to draft it.\n\n> **Key Insight**: The acceptance probability formula has elegant properties:\n> - If $p_t(x) \\geq p_d(x)$ (target likes this token more than draft): **always accept**\n> - If $p_t(x) < p_d(x)$ (draft is \"over-confident\"): accept proportionally\n> - The residual distribution captures what the target \"wants but draft doesn't provide\"\n\n> **Interview Talking Point**: When asked \"why does speculative decoding maintain output quality?\", explain that rejection sampling mathematically guarantees the marginal distribution of accepted tokens matches the target model exactly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_logits(logits: torch.Tensor, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample a token from logits and return (token, probability).\n",
    "    \n",
    "    Args:\n",
    "        logits: Shape (batch, vocab_size)\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (sampled_token, probability_of_sampled_token)\n",
    "    \"\"\"\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "    token = torch.multinomial(probs, num_samples=1)\n",
    "    token_prob = probs.gather(-1, token)\n",
    "    return token.squeeze(-1), token_prob.squeeze(-1)\n",
    "\n",
    "\n",
    "def speculative_sample(\n",
    "    draft_probs: torch.Tensor,\n",
    "    target_probs: torch.Tensor,\n",
    "    draft_tokens: torch.Tensor\n",
    ") -> Tuple[int, Optional[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Perform rejection sampling on a sequence of drafted tokens.\n",
    "    \n",
    "    This implements Algorithm 1 from Leviathan et al. (2023).\n",
    "    \n",
    "    Args:\n",
    "        draft_probs: Probabilities assigned by draft model, shape (n_drafted, vocab)\n",
    "        target_probs: Probabilities assigned by target model, shape (n_drafted, vocab)\n",
    "        draft_tokens: The tokens sampled by the draft model, shape (n_drafted,)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of:\n",
    "        - n_accepted: Number of tokens accepted (0 to n_drafted)\n",
    "        - correction_token: If rejected early, the corrected token from target dist\n",
    "    \"\"\"\n",
    "    n_drafted = draft_tokens.shape[0]\n",
    "    device = draft_tokens.device\n",
    "    \n",
    "    for i in range(n_drafted):\n",
    "        token = draft_tokens[i]\n",
    "        \n",
    "        # Get probabilities for this token\n",
    "        p_draft = draft_probs[i, token]\n",
    "        p_target = target_probs[i, token]\n",
    "        \n",
    "        # Acceptance probability\n",
    "        accept_prob = torch.min(torch.ones(1, device=device), p_target / (p_draft + 1e-10))\n",
    "        \n",
    "        # Rejection sampling\n",
    "        r = torch.rand(1, device=device)\n",
    "        \n",
    "        if r >= accept_prob:\n",
    "            # Rejected! Sample from residual distribution\n",
    "            # p_residual = max(0, p_target - p_draft)\n",
    "            residual = torch.clamp(target_probs[i] - draft_probs[i], min=0)\n",
    "            residual_sum = residual.sum()\n",
    "            \n",
    "            if residual_sum > 1e-10:\n",
    "                residual = residual / residual_sum\n",
    "                correction_token = torch.multinomial(residual, num_samples=1)\n",
    "            else:\n",
    "                # Fallback to target distribution\n",
    "                correction_token = torch.multinomial(target_probs[i], num_samples=1)\n",
    "            \n",
    "            return i, correction_token.squeeze()\n",
    "    \n",
    "    # All tokens accepted! Sample one more from target\n",
    "    # (This is the \"bonus\" token from speculative decoding)\n",
    "    return n_drafted, None\n",
    "\n",
    "\n",
    "# Test speculative sampling\n",
    "print(\"Testing speculative sampling...\")\n",
    "vocab_size = 100\n",
    "n_tokens = 5\n",
    "\n",
    "# Create mock probabilities\n",
    "draft_probs = F.softmax(torch.randn(n_tokens, vocab_size), dim=-1)\n",
    "target_probs = F.softmax(torch.randn(n_tokens, vocab_size), dim=-1)\n",
    "draft_tokens = torch.multinomial(draft_probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "n_accepted, correction = speculative_sample(draft_probs, target_probs, draft_tokens)\n",
    "print(f\"Drafted {n_tokens} tokens, accepted {n_accepted}\")\n",
    "if correction is not None:\n",
    "    print(f\"Correction token: {correction.item()}\")\n",
    "print(\"Speculative sampling tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "def visualize_rejection_sampling_step(draft_probs, target_probs, token_idx, drafted_token, tokenizer, accept_threshold=None):\n    \"\"\"\n    Visualize a single rejection sampling step with probability comparison.\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n    \n    # Get top tokens for visualization\n    top_k = 10\n    draft_top_probs, draft_top_indices = torch.topk(draft_probs, top_k)\n    target_top_probs, target_top_indices = torch.topk(target_probs, top_k)\n    \n    # Combine to get interesting tokens to show\n    combined_indices = torch.unique(torch.cat([draft_top_indices, target_top_indices]))[:12]\n    \n    draft_vals = draft_probs[combined_indices].cpu().numpy()\n    target_vals = target_probs[combined_indices].cpu().numpy()\n    token_labels = [tokenizer.decode([idx.item()]).strip() or f\"[{idx.item()}]\" for idx in combined_indices]\n    \n    # Highlight the drafted token\n    drafted_in_combined = (combined_indices == drafted_token).nonzero()\n    highlight_idx = drafted_in_combined[0].item() if len(drafted_in_combined) > 0 else None\n    \n    # Plot 1: Draft vs Target distributions\n    ax1 = axes[0]\n    x = np.arange(len(combined_indices))\n    width = 0.35\n    \n    bars1 = ax1.bar(x - width/2, draft_vals, width, label='Draft $p_d(x)$', color='#3498db', alpha=0.8)\n    bars2 = ax1.bar(x + width/2, target_vals, width, label='Target $p_t(x)$', color='#e74c3c', alpha=0.8)\n    \n    if highlight_idx is not None:\n        bars1[highlight_idx].set_edgecolor('black')\n        bars1[highlight_idx].set_linewidth(3)\n        bars2[highlight_idx].set_edgecolor('black')\n        bars2[highlight_idx].set_linewidth(3)\n    \n    ax1.set_ylabel('Probability')\n    ax1.set_title('Draft vs Target Probability Distributions')\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=9)\n    ax1.legend()\n    ax1.grid(axis='y', alpha=0.3)\n    \n    # Plot 2: Acceptance probability calculation\n    ax2 = axes[1]\n    p_d = draft_probs[drafted_token].item()\n    p_t = target_probs[drafted_token].item()\n    accept_prob = min(1.0, p_t / (p_d + 1e-10))\n    \n    drafted_token_str = tokenizer.decode([drafted_token.item()]).strip() or f\"[{drafted_token.item()}]\"\n    \n    ax2.barh([0], [1.0], color='#95a5a6', alpha=0.3, label='Rejection zone')\n    ax2.barh([0], [accept_prob], color='#2ecc71', alpha=0.8, label=f'Accept zone ({accept_prob:.2%})')\n    \n    if accept_threshold is not None:\n        ax2.axvline(x=accept_threshold, color='red', linestyle='--', linewidth=2, \n                   label=f'Random draw: {accept_threshold:.2f}')\n        result = \"ACCEPT\" if accept_threshold < accept_prob else \"REJECT\"\n        result_color = '#2ecc71' if accept_threshold < accept_prob else '#e74c3c'\n        ax2.text(0.5, 0.3, result, transform=ax2.transAxes, fontsize=24, fontweight='bold',\n                ha='center', color=result_color)\n    \n    ax2.set_xlim(0, 1)\n    ax2.set_ylim(-0.5, 0.5)\n    ax2.set_yticks([])\n    ax2.set_xlabel('Probability')\n    ax2.set_title(f'Acceptance for \"{drafted_token_str}\"\\n$p_d$={p_d:.4f}, $p_t$={p_t:.4f}')\n    ax2.legend(loc='upper right')\n    \n    # Plot 3: Residual distribution\n    ax3 = axes[2]\n    residual = torch.clamp(target_probs - draft_probs, min=0)\n    residual_sum = residual.sum().item()\n    \n    if residual_sum > 1e-6:\n        residual_normalized = residual / residual_sum\n        residual_vals = residual_normalized[combined_indices].cpu().numpy()\n        colors = ['#9b59b6' if v > 0.01 else '#d5d5d5' for v in residual_vals]\n        ax3.bar(x, residual_vals, color=colors, alpha=0.8)\n        ax3.set_ylabel('Probability')\n        ax3.set_title('Residual Distribution $\\\\max(0, p_t - p_d)$\\n(normalized)')\n        ax3.set_xticks(x)\n        ax3.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=9)\n        ax3.grid(axis='y', alpha=0.3)\n    else:\n        ax3.text(0.5, 0.5, 'No residual\\n(draft dominates target)', \n                transform=ax3.transAxes, ha='center', va='center', fontsize=12)\n        ax3.set_title('Residual Distribution')\n    \n    plt.tight_layout()\n    return fig\n\n\n# Demo: Visualize rejection sampling with mock data\nprint(\"Demonstrating rejection sampling visualization...\")\nvocab_size = tokenizer.vocab_size\n\n# Create realistic probability distributions\ndraft_logits = torch.randn(vocab_size) * 0.5\ndraft_logits[256] = 3.0  # Make one token very likely\ndraft_probs_demo = F.softmax(draft_logits, dim=-1)\n\ntarget_logits = torch.randn(vocab_size) * 0.5\ntarget_logits[512] = 2.5  # Different preferred token\ntarget_logits[256] = 1.5  # Still likes draft's token, but less\ntarget_probs_demo = F.softmax(target_logits, dim=-1)\n\ndrafted_token_demo = torch.tensor(256)\nrandom_draw = 0.85  # This will likely cause rejection\n\nfig = visualize_rejection_sampling_step(\n    draft_probs_demo, target_probs_demo, 0, drafted_token_demo, \n    tokenizer, accept_threshold=random_draw\n)\nplt.savefig('rejection_sampling_step.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nIn this example:\")\nprint(f\"  Draft probability for chosen token: {draft_probs_demo[256]:.4f}\")\nprint(f\"  Target probability for chosen token: {target_probs_demo[256]:.4f}\")\nprint(f\"  Acceptance probability: {min(1.0, target_probs_demo[256].item() / draft_probs_demo[256].item()):.2%}\")\nprint(f\"  Random draw: {random_draw:.2f}\")\nprint(f\"  Result: {'ACCEPT' if random_draw < min(1.0, target_probs_demo[256].item() / draft_probs_demo[256].item()) else 'REJECT'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Visualizing Rejection Sampling\n\nLet's create a visual demonstration of how rejection sampling works at the token level.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 4: The Velocity Engine\n\nThe main generation loop with adaptive γ (lookahead length).\n\n### Adaptive Strategy (Dual-Signal)\n\nWe use **two signals** to adapt γ:\n\n1. **Acceptance Rate**: If >80% accepted → increase γ, if <30% → decrease γ\n2. **Draft Entropy**: If the draft model is confident (low entropy) → draft more tokens\n\n### KV Cache with Proper Rollback\n\nUnlike naive implementations that recompute the entire cache, we use **incremental updates with rollback**:\n- After verification, we truncate the cache to only include accepted tokens\n- This is O(1) - we just slice the tensor, no recomputation needed\n\n### Enhanced Tracking\n\nWe now track **every token-level event** for detailed visualization:\n- Each drafted token and its probabilities\n- Accept/reject decisions with the exact acceptance probability\n- Correction tokens when rejection occurs\n- Cache position changes (rollbacks)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def compute_entropy(probs: torch.Tensor) -> torch.Tensor:\n    \"\"\"Compute entropy of a probability distribution.\"\"\"\n    return -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)\n\n\ndef truncate_kv_cache(past_key_values, seq_len: int):\n    \"\"\"\n    Truncate HuggingFace KV cache to a specific sequence length.\n    This is our 'rollback' operation - O(1) slicing, no recomputation.\n    \"\"\"\n    if past_key_values is None:\n        return None\n    return tuple(\n        (k[:, :, :seq_len, :], v[:, :, :seq_len, :])\n        for k, v in past_key_values\n    )\n\n\n@dataclass\nclass TokenEvent:\n    \"\"\"Detailed information about a single token decision.\"\"\"\n    iteration: int\n    position: int  # Position within the drafted sequence (0 to gamma-1)\n    drafted_token_id: int\n    drafted_token_str: str\n    draft_prob: float\n    target_prob: float\n    acceptance_prob: float\n    random_draw: float\n    accepted: bool\n    correction_token_id: Optional[int] = None\n    correction_token_str: Optional[str] = None\n\n\n@dataclass \nclass IterationEvent:\n    \"\"\"Information about a full iteration of speculative decoding.\"\"\"\n    iteration: int\n    gamma: int\n    tokens_drafted: List[str]\n    tokens_accepted: List[str]\n    n_accepted: int\n    correction_token: Optional[str]\n    bonus_token: Optional[str]\n    cache_pos_before: int\n    cache_pos_after: int\n    rollback_amount: int\n    avg_entropy: float\n    acceptance_rate: float\n    draft_time_ms: float\n    verify_time_ms: float\n\n\n@dataclass\nclass VelocityStats:\n    \"\"\"Enhanced statistics from a generation run with detailed event tracking.\"\"\"\n    total_tokens: int = 0\n    total_drafted: int = 0\n    total_accepted: int = 0\n    gamma_history: List[int] = None\n    acceptance_history: List[float] = None\n    entropy_history: List[float] = None\n    rollback_history: List[int] = None\n    cache_positions: List[int] = None\n    wall_time: float = 0.0\n    \n    # Enhanced tracking\n    token_events: List[TokenEvent] = None\n    iteration_events: List[IterationEvent] = None\n    draft_times: List[float] = None\n    verify_times: List[float] = None\n    \n    def __post_init__(self):\n        if self.gamma_history is None:\n            self.gamma_history = []\n        if self.acceptance_history is None:\n            self.acceptance_history = []\n        if self.entropy_history is None:\n            self.entropy_history = []\n        if self.rollback_history is None:\n            self.rollback_history = []\n        if self.cache_positions is None:\n            self.cache_positions = []\n        if self.token_events is None:\n            self.token_events = []\n        if self.iteration_events is None:\n            self.iteration_events = []\n        if self.draft_times is None:\n            self.draft_times = []\n        if self.verify_times is None:\n            self.verify_times = []\n    \n    @property\n    def acceptance_rate(self) -> float:\n        if self.total_drafted == 0:\n            return 0.0\n        return self.total_accepted / self.total_drafted\n    \n    @property\n    def tokens_per_second(self) -> float:\n        if self.wall_time == 0:\n            return 0.0\n        return self.total_tokens / self.wall_time\n    \n    @property\n    def total_draft_time(self) -> float:\n        return sum(self.draft_times)\n    \n    @property\n    def total_verify_time(self) -> float:\n        return sum(self.verify_times)\n\n\ndef velocity_generate(\n    prompt: str,\n    target_model,\n    draft_model,\n    tokenizer,\n    max_new_tokens: int = 100,\n    initial_gamma: int = 4,\n    min_gamma: int = 1,\n    max_gamma: int = 8,\n    temperature: float = 1.0,\n    adaptive: bool = True,\n    use_entropy: bool = True,\n    entropy_threshold: float = 2.0,\n    verbose: bool = False,\n    track_details: bool = True  # NEW: Enable detailed token tracking\n) -> Tuple[str, VelocityStats]:\n    \"\"\"\n    Generate text using adaptive speculative decoding with proper cache rollback.\n    \n    Args:\n        prompt: Input text\n        target_model: The large, high-quality model\n        draft_model: The small, fast model\n        tokenizer: Tokenizer for both models\n        max_new_tokens: Maximum tokens to generate\n        initial_gamma: Starting lookahead length\n        min_gamma: Minimum γ\n        max_gamma: Maximum γ\n        temperature: Sampling temperature\n        adaptive: Whether to adapt γ based on acceptance rate\n        use_entropy: Whether to also use draft entropy for adaptation\n        entropy_threshold: Entropy below which draft is considered \"confident\"\n        verbose: Print debug info\n        track_details: Enable detailed per-token event tracking\n    \n    Returns:\n        Tuple of (generated_text, statistics)\n    \"\"\"\n    device = next(target_model.parameters()).device\n    stats = VelocityStats()\n    \n    # Encode prompt\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    prompt_len = input_ids.shape[1]\n    generated_ids = input_ids.clone()\n    \n    gamma = initial_gamma\n    start_time = time.time()\n    \n    # Initialize KV caches with first forward pass\n    with torch.no_grad():\n        target_out = target_model(input_ids, use_cache=True)\n        target_past = target_out.past_key_values\n        \n        draft_out = draft_model(input_ids, use_cache=True)\n        draft_past = draft_out.past_key_values\n    \n    cache_pos = prompt_len\n    stats.cache_positions.append(cache_pos)\n    \n    n_generated = 0\n    iteration = 0\n    \n    while n_generated < max_new_tokens:\n        iteration += 1\n        stats.gamma_history.append(gamma)\n        cache_pos_before = cache_pos\n        \n        # ============ DRAFT PHASE ============\n        draft_start = time.time()\n        draft_tokens = []\n        draft_probs_list = []\n        draft_entropies = []\n        current_token = generated_ids[:, -1:]\n        temp_draft_past = draft_past\n        \n        with torch.no_grad():\n            for _ in range(gamma):\n                draft_out = draft_model(\n                    current_token,\n                    past_key_values=temp_draft_past,\n                    use_cache=True\n                )\n                logits = draft_out.logits[:, -1, :]\n                probs = F.softmax(logits / temperature, dim=-1)\n                \n                entropy = compute_entropy(probs.squeeze())\n                draft_entropies.append(entropy.item())\n                \n                next_token = torch.multinomial(probs, num_samples=1)\n                \n                draft_tokens.append(next_token.squeeze())\n                draft_probs_list.append(probs.squeeze())\n                \n                current_token = next_token\n                temp_draft_past = draft_out.past_key_values\n        \n        draft_time = (time.time() - draft_start) * 1000\n        stats.draft_times.append(draft_time)\n        \n        draft_tokens = torch.stack(draft_tokens)\n        draft_probs = torch.stack(draft_probs_list)\n        avg_entropy = np.mean(draft_entropies)\n        stats.entropy_history.append(avg_entropy)\n        \n        # Get token strings for tracking\n        drafted_token_strs = [tokenizer.decode([t.item()]) for t in draft_tokens]\n        \n        # ============ VERIFY PHASE ============\n        verify_start = time.time()\n        draft_sequence = draft_tokens.unsqueeze(0)\n        \n        with torch.no_grad():\n            verify_input = torch.cat([generated_ids[:, -1:], draft_sequence], dim=1)\n            target_out = target_model(\n                verify_input,\n                past_key_values=target_past,\n                use_cache=True\n            )\n        \n        verify_time = (time.time() - verify_start) * 1000\n        stats.verify_times.append(verify_time)\n        \n        target_logits = target_out.logits[:, :-1, :]\n        target_probs = F.softmax(target_logits.squeeze(0) / temperature, dim=-1)\n        \n        # ============ ACCEPT/REJECT with detailed tracking ============\n        n_accepted = 0\n        correction = None\n        \n        for i in range(gamma):\n            token = draft_tokens[i]\n            p_draft = draft_probs[i, token].item()\n            p_target = target_probs[i, token].item()\n            accept_prob = min(1.0, p_target / (p_draft + 1e-10))\n            random_draw = torch.rand(1, device=device).item()\n            \n            accepted = random_draw < accept_prob\n            \n            if track_details:\n                event = TokenEvent(\n                    iteration=iteration,\n                    position=i,\n                    drafted_token_id=token.item(),\n                    drafted_token_str=drafted_token_strs[i],\n                    draft_prob=p_draft,\n                    target_prob=p_target,\n                    acceptance_prob=accept_prob,\n                    random_draw=random_draw,\n                    accepted=accepted\n                )\n            \n            if not accepted:\n                # Rejected - sample from residual\n                residual = torch.clamp(target_probs[i] - draft_probs[i], min=0)\n                residual_sum = residual.sum()\n                \n                if residual_sum > 1e-10:\n                    residual = residual / residual_sum\n                    correction = torch.multinomial(residual, num_samples=1).squeeze()\n                else:\n                    correction = torch.multinomial(target_probs[i], num_samples=1).squeeze()\n                \n                if track_details:\n                    event.correction_token_id = correction.item()\n                    event.correction_token_str = tokenizer.decode([correction.item()])\n                    stats.token_events.append(event)\n                break\n            else:\n                n_accepted += 1\n                if track_details:\n                    stats.token_events.append(event)\n        \n        stats.total_drafted += gamma\n        stats.total_accepted += n_accepted\n        \n        iter_acceptance = n_accepted / gamma\n        stats.acceptance_history.append(iter_acceptance)\n        \n        # ============ ROLLBACK CALCULATION ============\n        n_rejected = gamma - n_accepted\n        stats.rollback_history.append(n_rejected)\n        \n        if verbose:\n            rollback_str = f\" ROLLBACK {n_rejected}\" if n_rejected > 0 else \"\"\n            print(f\"[Iter {iteration}] γ={gamma}, accepted={n_accepted}/{gamma} ({iter_acceptance:.0%}){rollback_str}, entropy={avg_entropy:.2f}\")\n        \n        # ============ UPDATE SEQUENCE ============\n        accepted_token_strs = drafted_token_strs[:n_accepted]\n        correction_str = None\n        bonus_str = None\n        \n        if n_accepted > 0:\n            accepted_tokens = draft_tokens[:n_accepted].unsqueeze(0)\n            generated_ids = torch.cat([generated_ids, accepted_tokens], dim=1)\n        \n        if correction is not None:\n            generated_ids = torch.cat([generated_ids, correction.unsqueeze(0).unsqueeze(0)], dim=1)\n            correction_str = tokenizer.decode([correction.item()])\n            n_new = n_accepted + 1\n        else:\n            bonus_logits = target_out.logits[:, -1, :]\n            bonus_probs = F.softmax(bonus_logits / temperature, dim=-1)\n            bonus_token = torch.multinomial(bonus_probs, num_samples=1)\n            generated_ids = torch.cat([generated_ids, bonus_token], dim=1)\n            bonus_str = tokenizer.decode([bonus_token.item()])\n            n_new = n_accepted + 1\n        \n        n_generated += n_new\n        stats.total_tokens = n_generated\n        \n        # ============ CACHE ROLLBACK ============\n        new_cache_pos = cache_pos + n_new\n        target_past = truncate_kv_cache(target_out.past_key_values, cache_pos + 1 + n_accepted + 1)\n        \n        draft_out = draft_model(generated_ids[:, cache_pos:], \n                                past_key_values=truncate_kv_cache(draft_past, cache_pos),\n                                use_cache=True)\n        draft_past = draft_out.past_key_values\n        \n        cache_pos = new_cache_pos\n        stats.cache_positions.append(cache_pos)\n        \n        # Track iteration event\n        if track_details:\n            iter_event = IterationEvent(\n                iteration=iteration,\n                gamma=gamma,\n                tokens_drafted=drafted_token_strs,\n                tokens_accepted=accepted_token_strs,\n                n_accepted=n_accepted,\n                correction_token=correction_str,\n                bonus_token=bonus_str,\n                cache_pos_before=cache_pos_before,\n                cache_pos_after=cache_pos,\n                rollback_amount=n_rejected,\n                avg_entropy=avg_entropy,\n                acceptance_rate=iter_acceptance,\n                draft_time_ms=draft_time,\n                verify_time_ms=verify_time\n            )\n            stats.iteration_events.append(iter_event)\n        \n        # ============ ADAPTIVE GAMMA ============\n        if adaptive:\n            if iter_acceptance > 0.8:\n                gamma = min(gamma + 1, max_gamma)\n            elif iter_acceptance < 0.3:\n                gamma = max(gamma - 1, min_gamma)\n            \n            if use_entropy and avg_entropy < entropy_threshold and iter_acceptance >= 0.5:\n                gamma = min(gamma + 1, max_gamma)\n        \n        if generated_ids[0, -1].item() == tokenizer.eos_token_id:\n            break\n    \n    stats.wall_time = time.time() - start_time\n    output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    \n    return output_text, stats\n\n\nprint(\"Velocity engine with enhanced tracking defined!\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test the velocity engine with new features\nprint(\"Testing Velocity Engine with Enhanced Tracking...\")\nprint(\"=\" * 60)\n\ntest_prompt = \"The future of artificial intelligence is\"\n\noutput, stats = velocity_generate(\n    test_prompt,\n    target_model,\n    draft_model,\n    tokenizer,\n    max_new_tokens=50,\n    initial_gamma=4,\n    adaptive=True,\n    use_entropy=True,\n    verbose=True,\n    track_details=True\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"Generated text:\\n{output}\")\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"Statistics:\")\nprint(f\"  Total tokens: {stats.total_tokens}\")\nprint(f\"  Acceptance rate: {stats.acceptance_rate:.1%}\")\nprint(f\"  Tokens/second: {stats.tokens_per_second:.1f}\")\nprint(f\"  Wall time: {stats.wall_time:.2f}s\")\nprint(f\"  Total rollbacks: {sum(stats.rollback_history)}\")\nprint(f\"  Average entropy: {np.mean(stats.entropy_history):.2f}\")\nprint(f\"  Token events tracked: {len(stats.token_events)}\")\nprint(f\"  Iteration events tracked: {len(stats.iteration_events)}\")"
  },
  {
   "cell_type": "code",
   "source": "def visualize_entropy_vs_acceptance(stats: VelocityStats):\n    \"\"\"Show relationship between draft entropy and acceptance rate.\"\"\"\n    if len(stats.iteration_events) < 3:\n        print(\"Not enough data for entropy analysis\")\n        return\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    entropies = [e.avg_entropy for e in stats.iteration_events]\n    acceptances = [e.acceptance_rate for e in stats.iteration_events]\n    gammas = [e.gamma for e in stats.iteration_events]\n    \n    # Scatter plot\n    ax1 = axes[0]\n    scatter = ax1.scatter(entropies, acceptances, c=gammas, cmap='viridis', \n                         s=100, alpha=0.7, edgecolors='black')\n    plt.colorbar(scatter, ax=ax1, label='γ (Lookahead)')\n    \n    z = np.polyfit(entropies, acceptances, 1)\n    p = np.poly1d(z)\n    entropy_range = np.linspace(min(entropies), max(entropies), 100)\n    ax1.plot(entropy_range, p(entropy_range), 'r--', linewidth=2, label=f'Trend (slope={z[0]:.2f})')\n    \n    ax1.set_xlabel('Draft Entropy')\n    ax1.set_ylabel('Acceptance Rate')\n    ax1.set_title('Entropy vs Acceptance Rate')\n    ax1.legend()\n    ax1.grid(alpha=0.3)\n    ax1.set_ylim(-0.05, 1.05)\n    \n    correlation = np.corrcoef(entropies, acceptances)[0, 1]\n    ax1.annotate(f'Correlation: {correlation:.2f}', xy=(0.02, 0.98), xycoords='axes fraction',\n                ha='left', va='top', fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n    \n    # Gamma adaptation visualization\n    ax2 = axes[1]\n    iterations = range(len(acceptances))\n    ax2.plot(iterations, acceptances, 'b-o', linewidth=2, markersize=6, label='Acceptance Rate')\n    ax2.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='Increase γ (0.8)')\n    ax2.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='Decrease γ (0.3)')\n    \n    for i in range(1, len(gammas)):\n        if gammas[i] > gammas[i-1]:\n            ax2.axvline(x=i, color='green', alpha=0.3, linewidth=8)\n        elif gammas[i] < gammas[i-1]:\n            ax2.axvline(x=i, color='red', alpha=0.3, linewidth=8)\n    \n    ax2_twin = ax2.twinx()\n    ax2_twin.plot(iterations, gammas, 'k--', linewidth=1.5, alpha=0.5)\n    ax2_twin.set_ylabel('γ (dashed)', color='gray')\n    \n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('Acceptance Rate')\n    ax2.set_title('Adaptive γ in Action')\n    ax2.legend(loc='lower left')\n    ax2.grid(alpha=0.3)\n    ax2.set_ylim(-0.05, 1.05)\n    \n    plt.tight_layout()\n    plt.savefig('entropy_acceptance.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nEntropy & Adaptation Analysis:\")\n    print(f\"  Correlation (entropy vs acceptance): {correlation:.2f}\")\n    print(f\"  γ range used: {min(gammas)} - {max(gammas)}\")\n    print(f\"  Times γ increased: {sum(1 for i in range(1, len(gammas)) if gammas[i] > gammas[i-1])}\")\n    print(f\"  Times γ decreased: {sum(1 for i in range(1, len(gammas)) if gammas[i] < gammas[i-1])}\")\n\n\nvisualize_entropy_vs_acceptance(stats)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def visualize_cache_rollback(stats: VelocityStats):\n    \"\"\"Visualize KV cache position changes and rollback events.\"\"\"\n    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n    \n    iterations = range(len(stats.iteration_events))\n    \n    # Plot 1: Cache position waterfall\n    ax1 = axes[0]\n    for i, event in enumerate(stats.iteration_events):\n        ax1.scatter(i, event.cache_pos_before, c='#3498db', s=80, zorder=3, marker='o')\n        attempted_pos = event.cache_pos_before + event.gamma + 1\n        ax1.scatter(i, attempted_pos, c='#95a5a6', s=40, zorder=2, marker='x', alpha=0.5)\n        \n        if event.rollback_amount > 0:\n            ax1.scatter(i, event.cache_pos_after, c='#e74c3c', s=80, zorder=3, marker='s')\n            ax1.annotate('', xy=(i, event.cache_pos_after), xytext=(i, attempted_pos),\n                        arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2))\n        else:\n            ax1.scatter(i, event.cache_pos_after, c='#2ecc71', s=80, zorder=3, marker='s')\n        \n        ax1.plot([i, i], [event.cache_pos_before, event.cache_pos_after], \n                c='#2ecc71' if event.rollback_amount == 0 else '#e74c3c', linewidth=2, alpha=0.7)\n    \n    ax1.set_xlabel('Iteration')\n    ax1.set_ylabel('Cache Position')\n    ax1.set_title('KV Cache Position Over Time\\n(Circles=start, Squares=end, X=attempted, Red arrows=rollback)')\n    ax1.grid(alpha=0.3)\n    \n    # Plot 2: Rollback amounts\n    ax2 = axes[1]\n    rollbacks = [e.rollback_amount for e in stats.iteration_events]\n    colors = ['#e74c3c' if r > 0 else '#2ecc71' for r in rollbacks]\n    ax2.bar(iterations, rollbacks, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('Tokens Rolled Back')\n    ax2.set_title('Rollback Amount Per Iteration')\n    ax2.grid(alpha=0.3, axis='y')\n    \n    # Plot 3: Cumulative drafted vs generated\n    ax3 = axes[2]\n    cum_drafted = np.cumsum([e.gamma for e in stats.iteration_events])\n    cum_accepted = np.cumsum([e.n_accepted for e in stats.iteration_events])\n    cum_corrections = np.cumsum([1 if e.correction_token else 0 for e in stats.iteration_events])\n    cum_bonus = np.cumsum([1 if e.bonus_token else 0 for e in stats.iteration_events])\n    cum_total = cum_accepted + cum_corrections + cum_bonus\n    \n    ax3.fill_between(iterations, 0, cum_drafted, alpha=0.3, color='#3498db', label='Drafted')\n    ax3.plot(iterations, cum_drafted, 'b--', linewidth=2, alpha=0.7)\n    ax3.fill_between(iterations, 0, cum_total, alpha=0.8, color='#2ecc71', label='Generated')\n    ax3.plot(iterations, cum_total, 'g-', linewidth=2)\n    \n    ax3.set_xlabel('Iteration')\n    ax3.set_ylabel('Cumulative Tokens')\n    ax3.set_title('Drafted vs Generated (Gap = wasted speculation)')\n    ax3.legend(loc='upper left')\n    ax3.grid(alpha=0.3)\n    \n    efficiency = cum_total[-1] / cum_drafted[-1] if cum_drafted[-1] > 0 else 0\n    ax3.annotate(f'Efficiency: {efficiency:.1%}', xy=(0.98, 0.05), xycoords='axes fraction',\n                ha='right', fontsize=12, fontweight='bold',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n    \n    plt.tight_layout()\n    plt.savefig('cache_rollback.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nCache Rollback Summary:\")\n    print(f\"  Total rollback events: {sum(1 for r in rollbacks if r > 0)}\")\n    print(f\"  Total tokens rolled back: {sum(rollbacks)}\")\n    print(f\"  Speculation efficiency: {efficiency:.1%}\")\n\n\ndef visualize_latency_breakdown(stats: VelocityStats):\n    \"\"\"Analyze where time is spent during generation.\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    total_draft = sum(stats.draft_times)\n    total_verify = sum(stats.verify_times)\n    total_overhead = max(0, (stats.wall_time * 1000) - total_draft - total_verify)\n    \n    # Pie chart\n    ax1 = axes[0, 0]\n    sizes = [total_draft, total_verify, total_overhead]\n    labels = [f'Draft\\n({total_draft:.0f}ms)', f'Verify\\n({total_verify:.0f}ms)', f'Overhead\\n({total_overhead:.0f}ms)']\n    colors = ['#3498db', '#e74c3c', '#95a5a6']\n    ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)\n    ax1.set_title('Overall Time Breakdown')\n    \n    # Per-iteration stacked bar\n    ax2 = axes[0, 1]\n    iterations = range(len(stats.draft_times))\n    ax2.bar(iterations, stats.draft_times, label='Draft', color='#3498db', alpha=0.8)\n    ax2.bar(iterations, stats.verify_times, bottom=stats.draft_times, label='Verify', color='#e74c3c', alpha=0.8)\n    ax2.set_xlabel('Iteration')\n    ax2.set_ylabel('Time (ms)')\n    ax2.set_title('Time Per Iteration')\n    ax2.legend()\n    ax2.grid(alpha=0.3, axis='y')\n    \n    # Draft time vs gamma\n    ax3 = axes[1, 0]\n    gammas = [e.gamma for e in stats.iteration_events]\n    ax3.scatter(gammas, stats.draft_times, c='#3498db', s=80, alpha=0.7, edgecolors='black')\n    z = np.polyfit(gammas, stats.draft_times, 1)\n    p = np.poly1d(z)\n    gamma_range = np.linspace(min(gammas), max(gammas), 100)\n    ax3.plot(gamma_range, p(gamma_range), 'r--', linewidth=2, label=f'Trend: {z[0]:.1f}ms/token')\n    ax3.set_xlabel('γ (Lookahead)')\n    ax3.set_ylabel('Draft Time (ms)')\n    ax3.set_title('Draft Time vs Lookahead')\n    ax3.legend()\n    ax3.grid(alpha=0.3)\n    \n    # Verify time histogram\n    ax4 = axes[1, 1]\n    ax4.hist(stats.verify_times, bins=15, color='#e74c3c', alpha=0.8, edgecolor='black')\n    ax4.axvline(np.mean(stats.verify_times), color='black', linestyle='--', linewidth=2, \n                label=f'Mean: {np.mean(stats.verify_times):.1f}ms')\n    ax4.set_xlabel('Verify Time (ms)')\n    ax4.set_ylabel('Count')\n    ax4.set_title('Verification Time Distribution')\n    ax4.legend()\n    ax4.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('latency_breakdown.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"\\nLatency Analysis:\")\n    print(f\"  Total draft time: {total_draft:.0f}ms ({total_draft/(stats.wall_time*1000)*100:.1f}%)\")\n    print(f\"  Total verify time: {total_verify:.0f}ms ({total_verify/(stats.wall_time*1000)*100:.1f}%)\")\n    print(f\"  Draft/Verify ratio: {total_draft/total_verify:.2f}\")\n\n\n# Run visualizations\nvisualize_cache_rollback(stats)\nvisualize_latency_breakdown(stats)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def print_token_trace(stats: VelocityStats, max_iterations: int = None):\n    \"\"\"Print a detailed token-by-token trace of the generation process.\"\"\"\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TOKEN-LEVEL TRACE\")\n    print(\"=\" * 80)\n    print(\"\\nLegend: [OK]=Accepted  [X]=Rejected  [->]=Correction  [+]=Bonus\")\n    print(\"-\" * 80)\n    \n    iterations = stats.iteration_events\n    if max_iterations:\n        iterations = iterations[:max_iterations]\n    \n    for event in iterations:\n        print(f\"\\n[Iter {event.iteration}] γ={event.gamma}, Cache: {event.cache_pos_before} -> {event.cache_pos_after}\")\n        print(f\"  Draft: {event.draft_time_ms:.1f}ms | Verify: {event.verify_time_ms:.1f}ms | Entropy: {event.avg_entropy:.2f}\")\n        print(f\"  Drafted:  {' '.join([f'\\\"{t}\\\"' for t in event.tokens_drafted])}\")\n        \n        result_parts = []\n        for i, tok in enumerate(event.tokens_drafted):\n            if i < event.n_accepted:\n                result_parts.append(f\"[OK]\\\"{tok}\\\"\")\n            elif i == event.n_accepted:\n                result_parts.append(f\"[X]\\\"{tok}\\\"\")\n        \n        print(f\"  Result:   {' '.join(result_parts)}\")\n        \n        if event.correction_token:\n            print(f\"  Correction: [->]\\\"{event.correction_token}\\\" (from residual)\")\n        if event.bonus_token:\n            print(f\"  Bonus:    [+]\\\"{event.bonus_token}\\\" (all {event.gamma} accepted!)\")\n        \n        if event.rollback_amount > 0:\n            print(f\"  ROLLBACK: {event.rollback_amount} positions\")\n    \n    print(\"\\n\" + \"=\" * 80)\n\n\ndef visualize_token_events(stats: VelocityStats, max_events: int = 50):\n    \"\"\"Create visual representation of token acceptance/rejection.\"\"\"\n    events = stats.token_events[:max_events]\n    if not events:\n        print(\"No token events to visualize\")\n        return\n    \n    fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n    \n    # Plot 1: Acceptance probability vs random draw\n    ax1 = axes[0]\n    positions = range(len(events))\n    accept_probs = [e.acceptance_prob for e in events]\n    random_draws = [e.random_draw for e in events]\n    accepted = [e.accepted for e in events]\n    \n    colors = ['#2ecc71' if a else '#e74c3c' for a in accepted]\n    \n    ax1.scatter(positions, accept_probs, c='#3498db', s=100, marker='_', linewidths=3, \n                label='Accept threshold', zorder=3)\n    ax1.scatter(positions, random_draws, c=colors, s=60, marker='o', \n                edgecolors='black', linewidths=0.5, zorder=4)\n    ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n    \n    ax1.set_xlabel('Token Index')\n    ax1.set_ylabel('Probability')\n    ax1.set_title('Token Acceptance Decisions\\n(Blue line = threshold, Circle = random draw)')\n    ax1.set_ylim(-0.05, 1.1)\n    ax1.legend(['Accept threshold', 'Accepted', 'Rejected'], loc='upper right')\n    ax1.grid(alpha=0.3)\n    \n    token_labels = [e.drafted_token_str.strip()[:8] for e in events]\n    ax1.set_xticks(positions)\n    ax1.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=8)\n    \n    # Plot 2: Probability ratio\n    ax2 = axes[1]\n    prob_ratios = [e.target_prob / (e.draft_prob + 1e-10) for e in events]\n    \n    bar_colors = ['#2ecc71' if r >= 1 else '#f39c12' if r >= 0.5 else '#e74c3c' for r in prob_ratios]\n    ax2.bar(positions, prob_ratios, color=bar_colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n    ax2.axhline(y=1.0, color='black', linestyle='-', linewidth=1.5, label='Equal probability')\n    \n    ax2.set_xlabel('Token Index')\n    ax2.set_ylabel('p_target / p_draft')\n    ax2.set_title('Target/Draft Probability Ratio (>1 = target favors more)')\n    ax2.set_xticks(positions)\n    ax2.set_xticklabels(token_labels, rotation=45, ha='right', fontsize=8)\n    ax2.legend()\n    ax2.grid(alpha=0.3, axis='y')\n    ax2.set_ylim(0, min(3, max(prob_ratios) * 1.1))\n    \n    plt.tight_layout()\n    plt.savefig('token_events.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\n# Display the trace\nprint_token_trace(stats, max_iterations=10)\nvisualize_token_events(stats)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 4.1: Token-Level Trace Visualization\n\nNow let's look at exactly what happened at each step of generation. This provides complete transparency into the speculative decoding process.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 5: Comprehensive Benchmarks & Analysis\n\nWe now run a comprehensive benchmark suite comparing three approaches:\n1. **Baseline**: Standard autoregressive generation with target model\n2. **Fixed γ**: Speculative decoding with constant lookahead\n3. **Adaptive γ**: Our method with dynamic lookahead\n\n### Benchmark Categories\n\n| Category | Description | Expected Behavior |\n|----------|-------------|-------------------|\n| **Factual** | Predictable completions | High acceptance, high speedup |\n| **Creative** | Open-ended generation | Medium acceptance, moderate speedup |\n| **Code** | Programming patterns | Variable - depends on context |\n| **Repetitive** | Patterns like counting | Very high acceptance |\n| **Dialogue** | Conversational text | Medium acceptance |\n| **Technical** | Domain-specific text | Lower acceptance (specialized vocab) |",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def baseline_generate(\n    prompt: str,\n    model,\n    tokenizer,\n    max_new_tokens: int = 100,\n    temperature: float = 1.0\n) -> Tuple[str, float, float]:\n    \"\"\"\n    Standard autoregressive generation (baseline).\n    \n    Returns:\n        Tuple of (text, wall_time, tokens_per_second)\n    \"\"\"\n    device = next(model.parameters()).device\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    start_time = time.time()\n    \n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    wall_time = time.time() - start_time\n    n_generated = output_ids.shape[1] - input_ids.shape[1]\n    \n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    \n    return output_text, wall_time, n_generated / wall_time if wall_time > 0 else 0\n\n\n# Extended test prompts covering different categories\ntest_prompts = {\n    'Factual': \"The capital of France is\",\n    'Creative': \"In a shocking turn of events, scientists discovered that\",\n    'Code': \"def fibonacci(n):\\n    '''Calculate the nth Fibonacci number'''\\n\",\n    'Repetitive': \"1, 2, 3, 4, 5,\",\n    'Dialogue': \"User: How are you today?\\nAssistant:\",\n    'Technical': \"The Transformer architecture uses self-attention mechanisms to\"\n}\n\nprint(\"Running comprehensive benchmarks...\")\nprint(\"=\" * 70)\nprint(\"This tests various prompt types to show how speculative decoding\")\nprint(\"performs across different scenarios.\")\nprint(\"=\" * 70)\n\n# Run benchmarks\nresults = {\n    'category': [],\n    'prompt': [],\n    'baseline_tps': [],\n    'fixed_gamma_tps': [],\n    'adaptive_tps': [],\n    'acceptance_rate': [],\n    'avg_gamma': [],\n    'speedup_fixed': [],\n    'speedup_adaptive': [],\n}\n\nfor category, prompt in tqdm(test_prompts.items(), desc=\"Benchmarking\"):\n    results['category'].append(category)\n    results['prompt'].append(prompt[:35] + \"...\" if len(prompt) > 35 else prompt)\n    \n    # Baseline\n    _, _, baseline_tps = baseline_generate(\n        prompt, target_model, tokenizer, max_new_tokens=50\n    )\n    results['baseline_tps'].append(baseline_tps)\n    \n    # Fixed gamma\n    _, stats_fixed = velocity_generate(\n        prompt, target_model, draft_model, tokenizer,\n        max_new_tokens=50, initial_gamma=4, adaptive=False, track_details=False\n    )\n    results['fixed_gamma_tps'].append(stats_fixed.tokens_per_second)\n    \n    # Adaptive gamma\n    _, stats_adaptive = velocity_generate(\n        prompt, target_model, draft_model, tokenizer,\n        max_new_tokens=50, initial_gamma=4, adaptive=True, track_details=False\n    )\n    results['adaptive_tps'].append(stats_adaptive.tokens_per_second)\n    results['acceptance_rate'].append(stats_adaptive.acceptance_rate)\n    results['avg_gamma'].append(np.mean(stats_adaptive.gamma_history))\n    \n    # Calculate speedups\n    results['speedup_fixed'].append(stats_fixed.tokens_per_second / baseline_tps if baseline_tps > 0 else 0)\n    results['speedup_adaptive'].append(stats_adaptive.tokens_per_second / baseline_tps if baseline_tps > 0 else 0)\n\nprint(\"\\nBenchmark complete!\")\nprint(\"\\n\" + \"=\" * 90)\nprint(\"BENCHMARK RESULTS SUMMARY\")\nprint(\"=\" * 90)\nprint(f\"\\n{'Category':<12} {'Baseline':>10} {'Fixed γ':>10} {'Adaptive':>10} {'Accept%':>10} {'Avg γ':>8} {'Speedup':>10}\")\nprint(f\"{'':12} {'(tok/s)':>10} {'(tok/s)':>10} {'(tok/s)':>10} {'':>10} {'':>8} {'(adapt)':>10}\")\nprint(\"-\" * 90)\n\nfor i in range(len(results['category'])):\n    print(f\"{results['category'][i]:<12} \"\n          f\"{results['baseline_tps'][i]:>10.1f} \"\n          f\"{results['fixed_gamma_tps'][i]:>10.1f} \"\n          f\"{results['adaptive_tps'][i]:>10.1f} \"\n          f\"{results['acceptance_rate'][i]:>9.0%} \"\n          f\"{results['avg_gamma'][i]:>8.1f} \"\n          f\"{results['speedup_adaptive'][i]:>9.2f}x\")\n\nprint(\"-\" * 90)\nprint(f\"{'AVERAGE':<12} \"\n      f\"{np.mean(results['baseline_tps']):>10.1f} \"\n      f\"{np.mean(results['fixed_gamma_tps']):>10.1f} \"\n      f\"{np.mean(results['adaptive_tps']):>10.1f} \"\n      f\"{np.mean(results['acceptance_rate']):>9.0%} \"\n      f\"{np.mean(results['avg_gamma']):>8.1f} \"\n      f\"{np.mean(results['speedup_adaptive']):>9.2f}x\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enhanced visualizations for benchmarks\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Grouped bar chart of tokens/second\nax1 = axes[0, 0]\nx = np.arange(len(results['category']))\nwidth = 0.25\n\nbars1 = ax1.bar(x - width, results['baseline_tps'], width, label='Baseline', color='#2ecc71')\nbars2 = ax1.bar(x, results['fixed_gamma_tps'], width, label='Fixed γ=4', color='#3498db')\nbars3 = ax1.bar(x + width, results['adaptive_tps'], width, label='Adaptive γ', color='#e74c3c')\n\nax1.set_ylabel('Tokens / Second')\nax1.set_title('Inference Speed by Prompt Category')\nax1.set_xticks(x)\nax1.set_xticklabels(results['category'], rotation=45, ha='right')\nax1.legend()\nax1.grid(axis='y', alpha=0.3)\n\n# Plot 2: Speedup comparison\nax2 = axes[0, 1]\nax2.bar(x - width/2, results['speedup_fixed'], width, label='Fixed γ=4', color='#3498db')\nax2.bar(x + width/2, results['speedup_adaptive'], width, label='Adaptive γ', color='#e74c3c')\nax2.axhline(y=1.0, color='gray', linestyle='--', label='Baseline (1.0x)')\n\nax2.set_ylabel('Speedup Factor')\nax2.set_title('Speedup vs Baseline')\nax2.set_xticks(x)\nax2.set_xticklabels(results['category'], rotation=45, ha='right')\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\n# Add speedup annotations\nfor i, (sf, sa) in enumerate(zip(results['speedup_fixed'], results['speedup_adaptive'])):\n    ax2.annotate(f'{sa:.2f}x', (i + width/2, sa + 0.05), ha='center', fontsize=9)\n\n# Plot 3: Acceptance rate by category\nax3 = axes[1, 0]\ncolors = plt.cm.RdYlGn([r for r in results['acceptance_rate']])\nbars = ax3.bar(results['category'], results['acceptance_rate'], color=colors, edgecolor='black')\nax3.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='High threshold')\nax3.axhline(y=0.3, color='red', linestyle='--', alpha=0.5, label='Low threshold')\n\nax3.set_ylabel('Acceptance Rate')\nax3.set_title('Token Acceptance Rate by Category\\n(Higher = better draft/target alignment)')\nax3.set_xticklabels(results['category'], rotation=45, ha='right')\nax3.legend()\nax3.grid(axis='y', alpha=0.3)\nax3.set_ylim(0, 1)\n\n# Plot 4: Acceptance rate vs Speedup correlation\nax4 = axes[1, 1]\nscatter = ax4.scatter(results['acceptance_rate'], results['speedup_adaptive'], \n                     c=results['avg_gamma'], cmap='viridis', s=200, \n                     edgecolors='black', linewidth=2)\nplt.colorbar(scatter, ax=ax4, label='Avg γ')\n\n# Add labels for each point\nfor i, cat in enumerate(results['category']):\n    ax4.annotate(cat, (results['acceptance_rate'][i], results['speedup_adaptive'][i]),\n                xytext=(5, 5), textcoords='offset points', fontsize=9)\n\n# Add trend line\nz = np.polyfit(results['acceptance_rate'], results['speedup_adaptive'], 1)\np = np.poly1d(z)\naccept_range = np.linspace(min(results['acceptance_rate']), max(results['acceptance_rate']), 100)\nax4.plot(accept_range, p(accept_range), 'r--', linewidth=2, alpha=0.7)\n\nax4.set_xlabel('Acceptance Rate')\nax4.set_ylabel('Speedup Factor')\nax4.set_title('Acceptance Rate vs Speedup\\n(Higher acceptance → better speedup)')\nax4.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('benchmark_comprehensive.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Print key insights\nprint(\"\\n\" + \"=\" * 70)\nprint(\"KEY INSIGHTS FROM BENCHMARKS\")\nprint(\"=\" * 70)\nprint(f\"\"\"\n1. BEST PERFORMANCE: {results['category'][np.argmax(results['speedup_adaptive'])]}\n   - Speedup: {max(results['speedup_adaptive']):.2f}x\n   - Acceptance Rate: {results['acceptance_rate'][np.argmax(results['speedup_adaptive'])]:.0%}\n\n2. WORST PERFORMANCE: {results['category'][np.argmin(results['speedup_adaptive'])]}\n   - Speedup: {min(results['speedup_adaptive']):.2f}x\n   - Acceptance Rate: {results['acceptance_rate'][np.argmin(results['speedup_adaptive'])]:.0%}\n\n3. ADAPTIVE vs FIXED:\n   - Adaptive wins in {sum(1 for sf, sa in zip(results['speedup_fixed'], results['speedup_adaptive']) if sa > sf)}/{len(results['category'])} categories\n   - Average improvement: {(np.mean(results['speedup_adaptive']) - np.mean(results['speedup_fixed'])):.2f}x\n\n4. CORRELATION: Acceptance rate vs speedup = {np.corrcoef(results['acceptance_rate'], results['speedup_adaptive'])[0,1]:.2f}\n   (Strong positive correlation confirms theory)\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 4.2: KV Cache Rollback Visualization\n\nThe key systems engineering insight: when tokens are rejected, we don't recompute the cache - we just truncate it. This is O(1) vs O(n).\n\nThe following visualization shows:\n1. Cache position over time (the \"pointer\")\n2. Rollback events (when tokens are rejected)\n3. The \"waterfall\" of cache state changes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def visualize_generated_tokens(stats: VelocityStats, prompt: str):\n    \"\"\"\n    Create a visual representation of where each token came from.\n    \n    Color coding:\n    - Green: Accepted from draft model\n    - Red/Orange: Correction token (draft rejected, sampled from residual)\n    - Blue: Bonus token (all drafted accepted, extra from target)\n    \"\"\"\n    from IPython.display import HTML, display\n    \n    # Build token source list\n    token_sources = []  # List of (token_str, source_type)\n    \n    for event in stats.iteration_events:\n        # Accepted tokens\n        for tok in event.tokens_accepted:\n            token_sources.append((tok, 'accepted'))\n        \n        # Correction token\n        if event.correction_token:\n            token_sources.append((event.correction_token, 'correction'))\n        \n        # Bonus token  \n        if event.bonus_token:\n            token_sources.append((event.bonus_token, 'bonus'))\n    \n    # Create HTML representation\n    html_parts = ['<div style=\"font-family: monospace; font-size: 14px; line-height: 1.8;\">']\n    html_parts.append('<p><strong>Prompt:</strong> ' + prompt + '</p>')\n    html_parts.append('<p><strong>Generated:</strong> ')\n    \n    color_map = {\n        'accepted': '#27ae60',    # Green\n        'correction': '#e74c3c',  # Red\n        'bonus': '#3498db'        # Blue\n    }\n    \n    for token, source in token_sources:\n        color = color_map[source]\n        # Escape HTML special characters\n        safe_token = token.replace('<', '&lt;').replace('>', '&gt;')\n        html_parts.append(\n            f'<span style=\"background-color: {color}; color: white; padding: 2px 4px; '\n            f'margin: 1px; border-radius: 3px; display: inline-block;\"'\n            f'title=\"{source}\">{safe_token}</span>'\n        )\n    \n    html_parts.append('</p>')\n    \n    # Legend\n    html_parts.append('<p style=\"margin-top: 20px;\"><strong>Legend:</strong> ')\n    html_parts.append('<span style=\"background-color: #27ae60; color: white; padding: 2px 8px; border-radius: 3px;\">Accepted (draft)</span> ')\n    html_parts.append('<span style=\"background-color: #e74c3c; color: white; padding: 2px 8px; border-radius: 3px;\">Correction (residual)</span> ')\n    html_parts.append('<span style=\"background-color: #3498db; color: white; padding: 2px 8px; border-radius: 3px;\">Bonus (target)</span>')\n    html_parts.append('</p></div>')\n    \n    display(HTML(''.join(html_parts)))\n    \n    # Also print statistics\n    n_accepted = sum(1 for _, s in token_sources if s == 'accepted')\n    n_correction = sum(1 for _, s in token_sources if s == 'correction')\n    n_bonus = sum(1 for _, s in token_sources if s == 'bonus')\n    total = len(token_sources)\n    \n    print(f\"\\nToken Source Breakdown:\")\n    print(f\"  Accepted (green):   {n_accepted:>3} ({n_accepted/total*100:>5.1f}%)\")\n    print(f\"  Correction (red):   {n_correction:>3} ({n_correction/total*100:>5.1f}%)\")\n    print(f\"  Bonus (blue):       {n_bonus:>3} ({n_bonus/total*100:>5.1f}%)\")\n    print(f\"  Total:              {total:>3}\")\n    \n    # Create a pie chart\n    fig, ax = plt.subplots(figsize=(8, 6))\n    sizes = [n_accepted, n_correction, n_bonus]\n    labels = [f'Accepted\\n({n_accepted})', f'Correction\\n({n_correction})', f'Bonus\\n({n_bonus})']\n    colors_pie = ['#27ae60', '#e74c3c', '#3498db']\n    explode = (0.02, 0.05, 0.02)\n    \n    ax.pie(sizes, explode=explode, labels=labels, colors=colors_pie, autopct='%1.1f%%',\n           shadow=True, startangle=90)\n    ax.set_title('Token Source Distribution\\n(Where did each output token come from?)')\n    \n    plt.savefig('token_sources.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\n# Run a longer generation for better visualization\nprint(\"Generating longer output for token source visualization...\")\nlong_output, long_stats = velocity_generate(\n    \"Once upon a time in a magical kingdom, there lived a\",\n    target_model,\n    draft_model,\n    tokenizer,\n    max_new_tokens=80,\n    initial_gamma=5,\n    adaptive=True,\n    verbose=False,\n    track_details=True\n)\n\nprint(f\"Generated {long_stats.total_tokens} tokens in {long_stats.wall_time:.2f}s\")\nprint(f\"({long_stats.tokens_per_second:.1f} tokens/sec)\\n\")\n\nvisualize_generated_tokens(long_stats, \"Once upon a time in a magical kingdom, there lived a\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 5.1: Visualizing the Generated Output\n\nLet's see exactly which tokens came from where in the final output. This provides intuition for how speculative decoding \"works\" at the token level.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Section 6: Summary & Key Takeaways\n\n### What We Built\n\nThis notebook demonstrates a **production-grade implementation** of Adaptive Speculative Decoding with comprehensive visualization and analysis tools.\n\n### Core Components Summary\n\n| Component | Purpose | Key Insight |\n|-----------|---------|-------------|\n| **KVCacheManager** | Pre-allocated cache with pointer | O(1) rollback vs O(n) recomputation |\n| **Rejection Sampling** | Accept/reject draft tokens | Guarantees exact target distribution |\n| **Adaptive γ** | Dynamic lookahead adjustment | Uses acceptance rate + entropy signals |\n| **Token Tracing** | Full visibility into decisions | Every accept/reject logged |\n\n### Algorithm Complexity\n\n| Operation | Naive | Our Implementation |\n|-----------|-------|-------------------|\n| Cache Rollback | O(seq_len) recompute | O(1) pointer move |\n| Draft K tokens | K forward passes | K forward passes |\n| Verify K tokens | K forward passes | 1 parallel pass |\n| **Amortized per token** | **2 passes** | **~(K+1)/K passes** |\n\n### Key Results from Benchmarks\n\n| Metric | Value | Notes |\n|--------|-------|-------|\n| **Best speedup** | Up to 2.5x | On predictable/repetitive content |\n| **Worst speedup** | ~1.0x | Graceful degradation on hard content |\n| **Quality loss** | None | Mathematically guaranteed |\n| **Memory overhead** | Minimal | Pre-allocated, reused buffers |\n\n### Interview Talking Points\n\n> **Q: What's the systems engineering challenge in speculative decoding?**\n> \n> A: Cache management. Naive implementations recompute O(n) on rejection. \n> We use pre-allocated buffers with pointer-based rollback for O(1).\n\n> **Q: How do you guarantee output quality?**\n> \n> A: Rejection sampling with residual distribution correction. Each token's\n> marginal probability matches the target model exactly.\n\n> **Q: When does speculative decoding fail?**\n> \n> A: When draft/target distributions diverge significantly. Our adaptive γ\n> detects this (low acceptance rate) and reduces speculation automatically.\n\n> **Q: What's the dual-signal adaptive strategy?**\n> \n> A: We use both acceptance rate (primary) and draft entropy (secondary).\n> Low entropy = confident draft = more speculation. High rejection = reduce γ.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Visualization Summary\n\nThis notebook provides extensive visualization tools:\n\n| Visualization | Purpose | Key Insight |\n|---------------|---------|-------------|\n| **Token Trace** | Shows every draft/accept/reject decision | Full transparency |\n| **Cache Rollback** | Visualizes O(1) pointer movement | Systems engineering proof |\n| **Rejection Sampling** | Shows probability comparison | Mathematical intuition |\n| **Latency Breakdown** | Time spent in draft vs verify | Verify dominates (as expected) |\n| **Entropy vs Acceptance** | Validates dual-signal hypothesis | Confident drafts accepted more |\n| **Benchmark Suite** | Tests across prompt categories | Real-world performance data |\n\n### References\n\n1. Leviathan, Y., Kalman, M., & Matias, Y. (2023). *Fast Inference from Transformers via Speculative Decoding*. ICML 2023.\n\n2. Chen, C., et al. (2023). *Accelerating Large Language Model Decoding with Speculative Sampling*. arXiv:2302.01318.\n\n3. SpecDec++ (2024). *Boosting Speculative Decoding via Adaptive Candidate Lengths*. arXiv preprint.\n\n### Potential Extensions\n\n| Extension | Description | Difficulty |\n|-----------|-------------|------------|\n| Tree Speculation | Draft multiple paths, select best | Medium |\n| Learned γ Predictor | Train model to predict optimal γ | Hard |\n| Quantized Models | Combine with 4-bit quantization | Medium |\n| Batch Speculation | Multiple prompts simultaneously | Hard |\n| Custom Draft Models | Distill from target for better alignment | Hard |\n\n---\n\n*Author: Matt McManus*  \n*Implementation: PyTorch native, no external inference frameworks*  \n*Lines of core algorithm code: ~200*  \n*Visualization code: ~400*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}